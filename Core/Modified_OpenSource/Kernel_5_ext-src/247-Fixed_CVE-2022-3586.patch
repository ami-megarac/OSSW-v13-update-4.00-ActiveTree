diff -Naur linux_org/net/sched/sch_sfb.c linux/net/sched/sch_sfb.c
--- linux_org/net/sched/sch_sfb.c	2022-12-20 16:42:16.058380309 +0800
+++ linux/net/sched/sch_sfb.c	2022-12-20 16:46:03.439592779 +0800
@@ -135,15 +135,15 @@
 	}
 }
 
-static void increment_qlen(const struct sk_buff *skb, struct sfb_sched_data *q)
+static void increment_qlen(const struct sfb_skb_cb *cb, struct sfb_sched_data *q)
 {
 	u32 sfbhash;
-
-	sfbhash = sfb_hash(skb, 0);
+	
+	sfbhash = cb->hashes[0];
 	if (sfbhash)
 		increment_one_qlen(sfbhash, 0, q);
-
-	sfbhash = sfb_hash(skb, 1);
+	
+	sfbhash = cb->hashes[1];
 	if (sfbhash)
 		increment_one_qlen(sfbhash, 1, q);
 }
@@ -281,8 +281,10 @@
 {
 
 	struct sfb_sched_data *q = qdisc_priv(sch);
+	unsigned int len = qdisc_pkt_len(skb);
 	struct Qdisc *child = q->qdisc;
 	struct tcf_proto *fl;
+	struct sfb_skb_cb cb;
 	int i;
 	u32 p_min = ~0;
 	u32 minqlen = ~0;
@@ -399,11 +401,12 @@
 	}
 
 enqueue:
+	memcpy(&cb, sfb_skb_cb(skb), sizeof(cb));
 	ret = qdisc_enqueue(skb, child, to_free);
-	if (likely(ret == NET_XMIT_SUCCESS)) {
-		qdisc_qstats_backlog_inc(sch, skb);
-		sch->q.qlen++;
-		increment_qlen(skb, q);
+	if (likely(ret == NET_XMIT_SUCCESS)) {		
+		sch->qstats.backlog += len;
+		sch->q.qlen++;		
+		increment_qlen(&cb, q);
 	} else if (net_xmit_drop_count(ret)) {
 		q->stats.childdrop++;
 		qdisc_qstats_drop(sch);
